{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exceptional-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import multiprocessing as mp\n",
    "\n",
    "OUTPUT = \"/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/clustering/\"\n",
    "INPUT = '/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/sent_embed/'\n",
    "NUM_CLUSTERS = 40\n",
    "\n",
    "#create list of numbers to represent months\n",
    "months = range(8)\n",
    "#load model\n",
    "model = Word2Vec.load('/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/dem_word2vec_100.model')\n",
    "#load means\n",
    "means = np.load('/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/clustering/dem_cluster_' + str(NUM_CLUSTERS) + '_means.npy')\n",
    "\n",
    "#cosine similarity function\n",
    "def cosine_sim(a, b):\n",
    "    return dot(a, b) / (norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "small-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that gets the scores/saves for each month\n",
    "def get_scores(m):\n",
    "    \n",
    "    #load cleaned text dataframe\n",
    "    text = pd.read_csv(INPUT + 'cleaned/dem_clean_' + str(m) + '.csv')\n",
    "    #load sentence embeddings\n",
    "    embedding = pd.read_csv(INPUT + 'trained/dem_trained_' + str(m) + '.csv')\n",
    "    \n",
    "    print('length of text: ' + str(len(text)))\n",
    "    print('length of embedding: ' + str(len(embedding)))\n",
    "    \n",
    "    #remove unnamed axxis from embedding\n",
    "    embedding = embedding.drop(['Unnamed: 0'], axis = 1)\n",
    "    \n",
    "    dicts = []\n",
    "    \n",
    "    for embed in range(len(embedding)):\n",
    "        d = {}\n",
    "        distances = np.array([cosine_sim(embedding.loc[embed], me) for me in means])\n",
    "        sorted_dists = distances.argsort()\n",
    "        for i, topic in enumerate(sorted_dists):\n",
    "            d['topic_' + str(i)] = topic  # ith closest topic\n",
    "            d['cosine_' + str(i)] = distances[topic]  # cosine distance of ith closest topic\n",
    "        dicts.append(d)\n",
    "    df = pd.DataFrame(dicts)\n",
    "    df.to_csv(OUTPUT + 'distances/dem_kmeans_topics_' + str(NUM_CLUSTERS) + '_' + str(m) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-thunder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 401951\n",
      "length of embedding: 401951\n",
      "length of text: 608384\n",
      "length of embedding: 608384\n",
      "length of text: 596779\n",
      "length of embedding: 596779\n",
      "length of text: 517796\n",
      "length of embedding: 517796\n",
      "length of text: 561750\n",
      "length of embedding: 561750\n",
      "length of text: 582628\n",
      "length of embedding: 582628\n",
      "length of text: 580913\n",
      "length of embedding: 580913\n",
      "length of text: 654338\n",
      "length of embedding: 654338\n"
     ]
    }
   ],
   "source": [
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "results = pool.map(get_scores, [m for m in months])\n",
    "\n",
    "pool.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-python_work]",
   "language": "python",
   "name": "conda-env-.conda-python_work-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
