{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create word2vec word embedding for reps using data already cleaned for glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "welcome-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import operator\n",
    "import random\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "guided-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = \"/home/twalton_umass_edu/Political Polarization Project/tmls/month_tmls_nrt/\"\n",
    "OUTPUT = \"/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/\"\n",
    "months = [\"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\", \"jan\"]\n",
    "RNG = random.Random()\n",
    "RNG.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "distributed-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for cleaning text\n",
    "#set the stemmer\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "#list of punctuation to be removed\n",
    "punct_chars = list((set(string.punctuation) | {'’', '‘', '–', '—', '~', '|', '“', '”', '…', \"'\", \"`\", '_'}) - set(['#']))\n",
    "#sort the punctuation\n",
    "punct_chars.sort()\n",
    "#make puncutation one string\n",
    "punctuation = ''.join(punct_chars)\n",
    "#symbols to be removed\n",
    "replace = re.compile('[%s]' % re.escape(punctuation))\n",
    "\n",
    "##########################################\n",
    "######function for cleaning text##########\n",
    "##########################################\n",
    "def clean_text(text, event=None, stem=True):\n",
    "    #remove emojis\n",
    "    text = re.sub('<U\\+[^>]+>', '', text)\n",
    "    #replace &amp; with and\n",
    "    text = re.sub('&amp;', 'and', text)\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    # eliminate urls\n",
    "    text = re.sub(r'http\\S*|\\S*\\.com\\S*|\\S*www\\S*', ' ', text)\n",
    "    #eliminate @mentions\n",
    "    text = re.sub(r'\\s@\\S+', ' ', text)\n",
    "    # substitute all other punctuation with whitespace\n",
    "    text = replace.sub(' ', text)\n",
    "    # replace all whitespace with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # strip off spaces on either end\n",
    "    text = text.strip()\n",
    "    # stem words\n",
    "    words = text.split()\n",
    "    if stem:\n",
    "        words = [sno.stem(w) for w in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "finished-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for cleaning tweet and keeping words in vocab set\n",
    "def clean_tweet(text):\n",
    "    cleaned = clean_text(text)\n",
    "    return ' '.join([w for w in cleaned if w in vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sunrise-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes df of all samples and returns vocab set\n",
    "def get_vocab(df):\n",
    "    word_counts = Counter(clean_text(' '.join(df['text'])))\n",
    "    vocab = []\n",
    "    for k, v in word_counts.items():\n",
    "        if v >= 10:\n",
    "            vocab.append(k)\n",
    "    return set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "legal-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for getting samples for developing a vocab\n",
    "def get_sample(df):\n",
    "    print('sample')\n",
    "    #get sample of 30000\n",
    "    return df.sample(min(len(df), 30000), random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "negative-crown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521899\n",
      "1471723\n",
      "1562853\n",
      "1496155\n",
      "1653814\n",
      "1618817\n",
      "1267603\n",
      "850552\n"
     ]
    }
   ],
   "source": [
    "#load the monthly data and filter to only republicans\n",
    "list_dfs = []\n",
    "\n",
    "for m in months:\n",
    "    df = pd.read_csv(INPUT + m + '_all_nrt_tweets.csv', encoding = 'UTF-8', dtype = 'str')\n",
    "    list_dfs.append(df[(df['dem_follows'] == '0') & (df['rep_follows'] == '1')]) #only append rep data\n",
    "    \n",
    "#print number of tweets in each dataframe\n",
    "for i, df in enumerate(list_dfs):\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "supreme-treasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11507728173814426\n",
      "0.1185882125916358\n",
      "0.12101649995233077\n",
      "0.12096273447604025\n",
      "0.13486764533375578\n",
      "0.14867832497434855\n",
      "0.14066075892846577\n",
      "0.12019841232517235\n"
     ]
    }
   ],
   "source": [
    "#keep only english tweets and remove the proportion of tweets removed for each month\n",
    "for i, df in enumerate(list_dfs):\n",
    "    #print proportion that is not english\n",
    "    print(len(df[df['lang'] != \"en\"]) / len(df))\n",
    "    #keep only english tweets in the list of dfs\n",
    "    list_dfs[i] = df[df['lang'] == \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "humanitarian-payroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "sample\n",
      "sample\n",
      "sample\n",
      "sample\n",
      "sample\n",
      "sample\n",
      "sample\n",
      "12612\n"
     ]
    }
   ],
   "source": [
    "#get vocabs\n",
    "\n",
    "#get montly samples, which are used to determine vocab\n",
    "samples_list = []\n",
    "\n",
    "for i, df in enumerate(list_dfs):\n",
    "    samples_list.append(get_sample(df))\n",
    "    \n",
    "#convert into a single dataframe\n",
    "samples = pd.concat(samples_list, ignore_index = True)\n",
    "del samples_list\n",
    "\n",
    "#get vocab\n",
    "vocab = get_vocab(samples)\n",
    "print(len(vocab))\n",
    "\n",
    "#save vocab\n",
    "with open(OUTPUT + 'rep_clean/joint_vocab.txt', 'w') as f:\n",
    "    f.write('\\n'.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "engaged-teach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cleaning :1346763\n",
      "after cleaning :1320012\n",
      "before cleaning :1297194\n",
      "after cleaning :1271015\n",
      "before cleaning :1373722\n",
      "after cleaning :1346811\n",
      "before cleaning :1315176\n",
      "after cleaning :1288970\n",
      "before cleaning :1430768\n",
      "after cleaning :1398755\n",
      "before cleaning :1378134\n",
      "after cleaning :1339435\n",
      "before cleaning :1089301\n",
      "after cleaning :1061463\n",
      "before cleaning :748317\n",
      "after cleaning :732420\n"
     ]
    }
   ],
   "source": [
    "#loop through dfs and clean text\n",
    "for i, df in enumerate(list_dfs):\n",
    "    list_dfs[i]['text'] = list_dfs[i]['text'].astype(str).apply(clean_tweet)\n",
    "    print('before cleaning :' + str(len(list_dfs[i])))\n",
    "    #drop tweets that are whitespace\n",
    "    list_dfs[i] = df[df['text'].str.contains(' ')]\n",
    "    print('after cleaning :' + str(len(list_dfs[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "immediate-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into a single data frame\n",
    "df = pd.concat(list_dfs, ignore_index = True)\n",
    "\n",
    "#write the cleaned text\n",
    "with open(OUTPUT + 'rep_clean/cleaned_text.txt', 'w') as f:\n",
    "    f.write('\\n'.join(df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "plain-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to a list of lists for feeding into the word2vec model\n",
    "final = [tweet.split() for tweet in df['text'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "focused-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "homeless-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=final, size=100, window=5, min_count=1, workers=12)\n",
    "\n",
    "model.save(OUTPUT + \"rep_word2vec_100.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
