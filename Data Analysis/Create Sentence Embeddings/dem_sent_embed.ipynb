{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "different-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dimensional-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG = random.Random()\n",
    "RNG.seed(400)\n",
    "\n",
    "INPUT = \"/home/twalton_umass_edu/Political Polarization Project/tmls/month_tmls/\"\n",
    "OUTPUT = \"/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/sent_embed/\"\n",
    "\n",
    "months = [\"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\", \"jan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expensive-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_weights(files, a=1e-3):\n",
    "    vectorizer = CountVectorizer(decode_error='ignore')\n",
    "    #get word frequencies\n",
    "    counts = vectorizer.fit_transform(files)\n",
    "    #get total # of times a word was used\n",
    "    total_freq = np.sum(counts, axis=0).T  # aggregate frequencies over all files\n",
    "    #number of words in the files\n",
    "    N = np.sum(total_freq)\n",
    "    #get the weighted frequency for each word\n",
    "    weighted_freq = a / (a + total_freq / N)\n",
    "    #garbage collection\n",
    "    gc.collect()\n",
    "    # dict with words and their weights\n",
    "    return dict(zip(vectorizer.get_feature_names(), weighted_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nasty-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences2idx(sentences, words2index, words2weight):\n",
    "    \"\"\"\n",
    "    Given a list of sentences, output array of word indices that can be fed into the algorithms.\n",
    "    :param sentences: a list of sentences\n",
    "    :param words: a dictionary, words['str'] is the indices of the word 'str'\n",
    "    :return: x1, m1. x1[i, :] is the word indices in sentence i, m1[i,:] is the mask for sentence i (0 means no word at the location)\n",
    "    \"\"\"\n",
    "    #print(sentences[0].split())\n",
    "    #get the longest tweet, capped at 100, used to create arrays\n",
    "    maxlen = min(max([len(s.split()) for s in sentences]), 150)\n",
    "    #print the length of longest tweet\n",
    "    print('maxlen', maxlen)\n",
    "    #get the number of sentences/tweets\n",
    "    n_samples = len(sentences)\n",
    "    #print the number of sentences\n",
    "    print('samples', n_samples)\n",
    "    #array for holding the word indices for each sentence\n",
    "    #each row is the sentence and each column contains the word indices\n",
    "    x = np.zeros((n_samples, maxlen)).astype('int32')\n",
    "    #same format as above but holds the weighted frequency for the words in each sentence\n",
    "    w = np.zeros((n_samples, maxlen)).astype('float32')\n",
    "    #dumy variable indicating whether the sentence has a word in that position\n",
    "    x_mask = np.zeros((n_samples, maxlen)).astype('float32')\n",
    "    #loop through sentences, idx = index of sentence, s = the sentence\n",
    "    for idx, s in enumerate(sentences):\n",
    "        #print for every 100000 sentences that have been indexed\n",
    "        if idx % 100000 == 0:\n",
    "            print(idx)\n",
    "        #split the sentence into tokens\n",
    "        split = s.split()\n",
    "        #list for holding the word indices\n",
    "        indices = []\n",
    "        #list for holding the weights of each word\n",
    "        weightlist = []\n",
    "        #loop through the words in the current sentence\n",
    "        for word in split:\n",
    "            #check if the word is in word embedding vector index\n",
    "            if word in words2index:\n",
    "                #if true, append the indices to the indices list\n",
    "                indices.append(words2index[word])\n",
    "                if word not in words2weight:\n",
    "                    #if the word does not have a weight, bcuz it was not in the sample used to get weights\n",
    "                    #give it this weight\n",
    "                    weightlist.append(0.000001)\n",
    "                else:\n",
    "                    #if it is in the list append its weighted frequency\n",
    "                    weightlist.append(words2weight[word])\n",
    "        #the length is the smalles of these two, indicates how many cells to fill in the three arrays\n",
    "        length = min(len(indices), maxlen)\n",
    "        #place all word indices in the row matching the index of the sentence\n",
    "        x[idx, :length] = indices[:length]\n",
    "        #place all weights into the array\n",
    "        w[idx, :length] = weightlist[:length]\n",
    "        #places ones in all cells indicating the length of the sentence\n",
    "        x_mask[idx, :length] = [1.] * length\n",
    "    #delete the sentences\n",
    "    del sentences\n",
    "    #clean the memory\n",
    "    gc.collect()\n",
    "    #return the three arrays\n",
    "    return x, x_mask, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smooth-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_average(We, x, m, w, dim):\n",
    "    \"\"\"\n",
    "    Compute the weighted average vectors\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in sentence i\n",
    "    :param w: w[i, :] are the weights for the words in sentence i\n",
    "    :return: emb[i, :] are the weighted average vector for sentence i\n",
    "    \"\"\"\n",
    "    print('Getting weighted average...')\n",
    "    n_samples = x.shape[0]\n",
    "    print(n_samples, dim)\n",
    "    #the sentence embeddings are the number of sentences by the number of dimensions, which is same as word embeddings\n",
    "    emb = np.zeros((n_samples, dim)).astype('float32')\n",
    "    \n",
    "    #loop through the sentences\n",
    "    for i in range(n_samples):\n",
    "        if i % 100000 == 0:\n",
    "            print(i)\n",
    "        #create list for holding each words 50 dimensional embedding\n",
    "        stacked = []\n",
    "        #loop through the word indices array for the current sentence\n",
    "        #idx = word position index, j = list of word indices\n",
    "        for idx, j in enumerate(x[i, :]):\n",
    "            #if there is not a word in the postion\n",
    "            if m[i, idx] != 1:\n",
    "                #append a row of zeros that is the length dim\n",
    "                stacked.append(np.zeros(dim))\n",
    "            else:\n",
    "                #append the word embedding for word indices j\n",
    "                stacked.append(We.wv[index2word[j]])\n",
    "        #create a numpy array by stacking the vectors\n",
    "        vectors = np.stack(stacked)\n",
    "        #emb[i,:] = w[i,:].dot(vectors) / np.count_nonzero(w[i,:])\n",
    "        nonzeros = np.sum(m[i,:])\n",
    "        #divide the dot product of the word weights and vectors by the sum of total words in the senteces\n",
    "        #creates a weighted average for each dimension by multiply the word dimension probablity by its weighted frequency\n",
    "        emb[i, :] = np.divide(w[i, :].dot(vectors), np.sum(m[i,:]), out=np.zeros(dim), where=nonzeros!=0)  # where there is a word\n",
    "    #clear memory\n",
    "    del x\n",
    "    del w\n",
    "    gc.collect()\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "composite-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the principal component of the sentence embeddings\n",
    "def compute_pc(X,npc=1):\n",
    "    \"\"\"\n",
    "    Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: component_[i,:] is the i-th pc\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "    print('Computing principal components...')\n",
    "    svd.fit(X)\n",
    "    return svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "other-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the principal component of the sentence embeddings\n",
    "def remove_pc(X, npc=1):\n",
    "    \"\"\"\n",
    "    Remove the projection on the principal components\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: XX[i, :] is the data point after removing its projection\n",
    "    \"\"\"\n",
    "    print('Removing principal component...')\n",
    "    pc = compute_pc(X, npc)\n",
    "    if npc == 1:\n",
    "        XX = X - X.dot(pc.transpose()) * pc\n",
    "    else:\n",
    "        XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "    return XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alternate-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the sentence embedding array\n",
    "def SIF_embedding(We, x, m, w, rmpc, dim):\n",
    "    \"\"\"\n",
    "    Compute the scores between pairs of sentences using weighted average + removing the projection on the first principal component\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in the i-th sentence\n",
    "    :param w: w[i, :] are the weights for the words in the i-th sentence\n",
    "    :param params.rmpc: if >0, remove the projections of the sentence embeddings to their first principal component\n",
    "    :return: emb, emb[i, :] is the embedding for sentence i\n",
    "    \"\"\"\n",
    "    emb = np.nan_to_num(get_weighted_average(We, x, m, w, dim))\n",
    "    if rmpc > 0:\n",
    "        emb = remove_pc(emb, rmpc)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "passive-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(docs, all_data, model, words2idx, dim, rmpc=1):\n",
    "    \"\"\"\n",
    "    :param docs: list of strings (i.e. docs), based on which to do the tf-idf weighting.\n",
    "    :param all_data: dataframe column / list of strings (all tweets)\n",
    "    :param model: pretrained word vectors\n",
    "    :param vocab: a dictionary, words['str'] is the indices of the word 'str'\n",
    "    :param dim: dimension of embeddings\n",
    "    :param rmpc: number of principal components to remove\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(dim)\n",
    "\n",
    "    print('Getting word weights...')\n",
    "    #get the word weights\n",
    "    word2weight = get_word_weights(docs)\n",
    "    # load sentences\n",
    "    print('Loading sentences...')\n",
    "    #get the sentence/word weight indices\n",
    "    x, m, w = sentences2idx(all_data, words2idx, word2weight)  # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "    print('Creating embeddings...')\n",
    "    #create and return the sentence embeddings\n",
    "    return SIF_embedding(model, x, m, w, rmpc, dim)  # embedding[i,:] is the embedding for sentence i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "coordinated-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the samples for computing the word weights\n",
    "def get_samples_for_computing_word_weights(p, sample_size):\n",
    "    tweets = []\n",
    "    with open('/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/' + p + '_clean/cleaned_text.txt', 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        tweets.extend([lines[i] for i in sorted(RNG.sample(range(len(lines)), min(sample_size, len(lines))))])\n",
    "    with open('/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/' + p +  '_tweets_for_weights.txt', 'w') as f:\n",
    "        f.write('\\n'.join(tweets))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "whole-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for cleaning text\n",
    "#set the stemmer\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "#list of punctuation to be removed\n",
    "punct_chars = list((set(string.punctuation) | {'’', '‘', '–', '—', '~', '|', '“', '”', '…', \"'\", \"`\", '_'}) - set(['#']))\n",
    "#sort the punctuation\n",
    "punct_chars.sort()\n",
    "#make puncutation one string\n",
    "punctuation = ''.join(punct_chars)\n",
    "#symbols to be removed\n",
    "replace = re.compile('[%s]' % re.escape(punctuation))\n",
    "\n",
    "##########################################\n",
    "######function for cleaning text##########\n",
    "##########################################\n",
    "def clean_text(text, event=None, stem=True):\n",
    "    #remove emojis\n",
    "    text = re.sub('<U\\+[^>]+>', '', text)\n",
    "    #replace &amp; with and\n",
    "    text = re.sub('&amp;', 'and', text)\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    # eliminate urls\n",
    "    text = re.sub(r'http\\S*|\\S*\\.com\\S*|\\S*www\\S*', ' ', text)\n",
    "    #eliminate @mentions\n",
    "    text = re.sub(r'\\s@\\S+', ' ', text)\n",
    "    # substitute all other punctuation with whitespace\n",
    "    text = replace.sub(' ', text)\n",
    "    # replace all whitespace with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # strip off spaces on either end\n",
    "    text = text.strip()\n",
    "    # stem words\n",
    "    words = text.split()\n",
    "    if stem:\n",
    "        words = [sno.stem(w) for w in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "entertaining-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for cleaning tweet and keeping words in vocab set\n",
    "def clean_tweet(text):\n",
    "    cleaned = clean_text(text)\n",
    "    return ' '.join([w for w in cleaned if w in vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "severe-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sentence embeddings for dems first\n",
    "#number of dimensions = 100\n",
    "d = 100\n",
    "\n",
    "#load in vectors\n",
    "vectors = Word2Vec.load('/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/dem_word2vec_100.model')\n",
    "\n",
    "#create an index that maps to the ordering of words in the word embedding vectors\n",
    "words2index = {w: i for i, w in enumerate(vectors.wv.vocab)}\n",
    "\n",
    "#for getting words from index\n",
    "index2word = {i: w for i, w in enumerate(vectors.wv.vocab)}\n",
    "\n",
    "#get sample of tweets for generating word weights, 30k * 7 months = 210K sample\n",
    "tweets_for_weights = get_samples_for_computing_word_weights(p = 'dem', sample_size = 240000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "perfect-magnitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13504"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in vocab for cleaning the full text\n",
    "vocab = vectors.wv.vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "united-politics",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644790\n",
      "632178\n",
      "660602\n",
      "629154\n",
      "720484\n",
      "624539\n",
      "442501\n",
      "562374\n"
     ]
    }
   ],
   "source": [
    "#load the monthly data and filter to only republicans\n",
    "list_dfs = []\n",
    "\n",
    "for m in months:\n",
    "    df = pd.read_csv(INPUT + m + '_all_tweets.csv', encoding = 'UTF-8', dtype = 'str')\n",
    "    list_dfs.append(df[(df['dem_follows'] == '1') & (df['rep_follows'] == '0')]) #only append rep data\n",
    "    \n",
    "#print number of tweets in each dataframe\n",
    "for i, df in enumerate(list_dfs):\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "latin-nashville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06423176538097675\n",
      "0.06816750978363689\n",
      "0.06839973236532738\n",
      "0.06623338642049482\n",
      "0.08073184137329906\n",
      "0.08639332371557261\n",
      "0.07786649069719616\n",
      "0.06645577498248496\n"
     ]
    }
   ],
   "source": [
    "#keep only english tweets and remove the proportion of tweets removed for each month\n",
    "for i, df in enumerate(list_dfs):\n",
    "    #print proportion that is not english\n",
    "    print(len(df[df['lang'] != \"en\"]) / len(df))\n",
    "    #keep only english tweets in the list of dfs\n",
    "    list_dfs[i] = df[df['lang'] == \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "artificial-toner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cleaning: 603374\n",
      "after cleaning: 596779\n",
      "before cleaning: 589084\n",
      "after cleaning: 582628\n",
      "before cleaning: 615417\n",
      "after cleaning: 608384\n",
      "before cleaning: 587483\n",
      "after cleaning: 580913\n",
      "before cleaning: 662318\n",
      "after cleaning: 654338\n",
      "before cleaning: 570583\n",
      "after cleaning: 561750\n",
      "before cleaning: 408045\n",
      "after cleaning: 401951\n",
      "before cleaning: 525001\n",
      "after cleaning: 517796\n"
     ]
    }
   ],
   "source": [
    "#loop through dfs and clean text\n",
    "for i, df in enumerate(list_dfs):\n",
    "    list_dfs[i]['text'] = list_dfs[i]['text'].astype(str).apply(clean_tweet)\n",
    "    print('before cleaning: ' + str(len(list_dfs[i])))\n",
    "    #drop tweets that are whitespace\n",
    "    list_dfs[i] = df[df['text'].str.contains(' ')]\n",
    "    print('after cleaning: ' + str(len(list_dfs[i])))\n",
    "    \n",
    "    #save as csv\n",
    "    list_dfs[i].to_csv(OUTPUT + 'cleaned/dem_clean_' + str(i) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stable-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "del list_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "planned-theme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Getting word weights...\n",
      "Loading sentences...\n",
      "maxlen 150\n",
      "samples 596779\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "Creating embeddings...\n",
      "Getting weighted average...\n",
      "596779 100\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "saving embedding...\n",
      "100\n",
      "Getting word weights...\n",
      "Loading sentences...\n",
      "maxlen 96\n",
      "samples 582628\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "Creating embeddings...\n",
      "Getting weighted average...\n",
      "582628 100\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "saving embedding...\n",
      "100\n",
      "Getting word weights...\n",
      "Loading sentences...\n",
      "maxlen 84\n",
      "samples 608384\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "Creating embeddings...\n",
      "Getting weighted average...\n",
      "608384 100\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "saving embedding...\n",
      "100\n",
      "Getting word weights...\n",
      "Loading sentences...\n",
      "maxlen 150\n",
      "samples 580913\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "Creating embeddings...\n",
      "Getting weighted average...\n",
      "580913 100\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "saving embedding...\n",
      "100\n",
      "Getting word weights...\n",
      "Loading sentences...\n",
      "maxlen 134\n",
      "samples 654338\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "Creating embeddings...\n",
      "Getting weighted average...\n",
      "654338 100\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "saving embedding...\n",
      "100\n",
      "Getting word weights...\n",
      "Loading sentences...\n",
      "maxlen 114\n",
      "samples 561750\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "Creating embeddings...\n",
      "Getting weighted average...\n",
      "561750 100\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "saving embedding...\n",
      "100\n",
      "Getting word weights...\n",
      "Loading sentences...\n",
      "maxlen 80\n",
      "samples 401951\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "Creating embeddings...\n",
      "Getting weighted average...\n",
      "401951 100\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "saving embedding...\n",
      "100\n",
      "Getting word weights...\n",
      "Loading sentences...\n",
      "maxlen 73\n",
      "samples 517796\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "Creating embeddings...\n",
      "Getting weighted average...\n",
      "517796 100\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "saving embedding...\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    df = pd.read_csv(OUTPUT + 'cleaned/dem_clean_' + str(i) + '.csv')\n",
    "    embedding = generate_embeddings(tweets_for_weights, df['text'], vectors, words2index, d)\n",
    "    embedding = pd.DataFrame(embedding, index = df.index)\n",
    "    print('saving embedding...')\n",
    "    embedding.to_csv(OUTPUT + 'trained/dem_trained_' + str(i) + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
