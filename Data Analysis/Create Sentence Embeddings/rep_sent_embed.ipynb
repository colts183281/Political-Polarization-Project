{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "important-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import random\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "peaceful-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG = random.Random()\n",
    "RNG.seed(400)\n",
    "\n",
    "INPUT = \"/home/twalton_umass_edu/Political Polarization Project/tmls/month_tmls/\"\n",
    "OUTPUT = \"/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/sent_embed/\"\n",
    "\n",
    "months = [\"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\", \"jan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prostate-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_weights(files, a=1e-3):\n",
    "    vectorizer = CountVectorizer(decode_error='ignore')\n",
    "    #get word frequencies\n",
    "    counts = vectorizer.fit_transform(files)\n",
    "    #get total # of times a word was used\n",
    "    total_freq = np.sum(counts, axis=0).T  # aggregate frequencies over all files\n",
    "    #number of words in the files\n",
    "    N = np.sum(total_freq)\n",
    "    #get the weighted frequency for each word\n",
    "    weighted_freq = a / (a + total_freq / N)\n",
    "    #garbage collection\n",
    "    gc.collect()\n",
    "    # dict with words and their weights\n",
    "    return dict(zip(vectorizer.get_feature_names(), weighted_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "grave-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences2idx(sentences, words2index, words2weight):\n",
    "    \"\"\"\n",
    "    Given a list of sentences, output array of word indices that can be fed into the algorithms.\n",
    "    :param sentences: a list of sentences\n",
    "    :param words: a dictionary, words['str'] is the indices of the word 'str'\n",
    "    :return: x1, m1. x1[i, :] is the word indices in sentence i, m1[i,:] is the mask for sentence i (0 means no word at the location)\n",
    "    \"\"\"\n",
    "    #print(sentences[0].split())\n",
    "    #get the longest tweet, capped at 105, used to create arrays\n",
    "    maxlen = min(max([len(s.split()) for s in sentences]), 150)\n",
    "    #print the length of longest tweet\n",
    "    print('maxlen', maxlen)\n",
    "    #get the number of sentences/tweets\n",
    "    n_samples = len(sentences)\n",
    "    #print the number of sentences\n",
    "    print('samples', n_samples)\n",
    "    #array for holding the word indices for each sentence\n",
    "    #each row is the sentence and each column contains the word indices\n",
    "    x = np.zeros((n_samples, maxlen)).astype('int32')\n",
    "    #same format as above but holds the weighted frequency for the words in each sentence\n",
    "    w = np.zeros((n_samples, maxlen)).astype('float32')\n",
    "    #dumy variable indicating whether the sentence has a word in that position\n",
    "    x_mask = np.zeros((n_samples, maxlen)).astype('float32')\n",
    "    #loop through sentences, idx = index of sentence, s = the sentence\n",
    "    for idx, s in enumerate(sentences):\n",
    "        #print for every 100000 sentences that have been indexed\n",
    "        if idx % 100000 == 0:\n",
    "            print(idx)\n",
    "        #split the sentence into tokens\n",
    "        split = s.split()\n",
    "        #list for holding the word indices\n",
    "        indices = []\n",
    "        #list for holding the weights of each word\n",
    "        weightlist = []\n",
    "        #loop through the words in the current sentence\n",
    "        for word in split:\n",
    "            #check if the word is in word embedding vector index\n",
    "            if word in words2index:\n",
    "                #if true, append the indices to the indices list\n",
    "                indices.append(words2index[word])\n",
    "                if word not in words2weight:\n",
    "                    #if the word does not have a weight, bcuz it was not in the sample used to get weights\n",
    "                    #give it this weight\n",
    "                    weightlist.append(0.000001)\n",
    "                else:\n",
    "                    #if it is in the list append its weighted frequency\n",
    "                    weightlist.append(words2weight[word])\n",
    "        #the length is the smalles of these two, indicates how many cells to fill in the three arrays\n",
    "        length = min(len(indices), maxlen)\n",
    "        #place all word indices in the row matching the index of the sentence\n",
    "        x[idx, :length] = indices[:length]\n",
    "        #place all weights into the array\n",
    "        w[idx, :length] = weightlist[:length]\n",
    "        #places ones in all cells indicating the length of the sentence\n",
    "        x_mask[idx, :length] = [1.] * length\n",
    "    #delete the sentences\n",
    "    del sentences\n",
    "    #clean the memory\n",
    "    gc.collect()\n",
    "    #return the three arrays\n",
    "    return x, x_mask, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "becoming-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_average(We, x, m, w, dim):\n",
    "    \"\"\"\n",
    "    Compute the weighted average vectors\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in sentence i\n",
    "    :param w: w[i, :] are the weights for the words in sentence i\n",
    "    :return: emb[i, :] are the weighted average vector for sentence i\n",
    "    \"\"\"\n",
    "    print('Getting weighted average...')\n",
    "    n_samples = x.shape[0]\n",
    "    print(n_samples, dim)\n",
    "    #the sentence embeddings are the number of sentences by the number of dimensions, which is same as word embeddings\n",
    "    emb = np.zeros((n_samples, dim)).astype('float32')\n",
    "    \n",
    "    #loop through the sentences\n",
    "    for i in range(n_samples):\n",
    "        if i % 100000 == 0:\n",
    "            print(i)\n",
    "        #create list for holding each words 50 dimensional embedding\n",
    "        stacked = []\n",
    "        #loop through the word indices array for the current sentence\n",
    "        #idx = word position index, j = list of word indices\n",
    "        for idx, j in enumerate(x[i, :]):\n",
    "            #if there is not a word in the postion\n",
    "            if m[i, idx] != 1:\n",
    "                #append a row of zeros that is the length dim\n",
    "                stacked.append(np.zeros(dim))\n",
    "            else:\n",
    "                #append the word embedding for word indices j\n",
    "                stacked.append(We.wv[index2word[j]])\n",
    "        #create a numpy array by stacking the vectors\n",
    "        vectors = np.stack(stacked)\n",
    "        nonzeros = np.sum(m[i,:])\n",
    "        #divide the dot product of the word weights and vectors by the sum of total words in the senteces\n",
    "        #creates a weighted average for each dimension by multiply the word dimension probablity by its weighted frequency\n",
    "        emb[i, :] = np.divide(w[i, :].dot(vectors), np.sum(m[i,:]), out=np.zeros(dim), where=nonzeros!=0)  # where there is a word\n",
    "    #clear memory\n",
    "    del x\n",
    "    del w\n",
    "    gc.collect()\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "promising-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the principal component of the sentence embeddings\n",
    "def compute_pc(X,npc=1):\n",
    "    \"\"\"\n",
    "    Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: component_[i,:] is the i-th pc\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "    print('Computing principal components...')\n",
    "    svd.fit(X)\n",
    "    return svd.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "immediate-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the principal component of the sentence embeddings\n",
    "def remove_pc(X, npc=1):\n",
    "    \"\"\"\n",
    "    Remove the projection on the principal components\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: XX[i, :] is the data point after removing its projection\n",
    "    \"\"\"\n",
    "    print('Removing principal component...')\n",
    "    pc = compute_pc(X, npc)\n",
    "    if npc == 1:\n",
    "        XX = X - X.dot(pc.transpose()) * pc\n",
    "    else:\n",
    "        XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "    return XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "seven-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the sentence embedding array\n",
    "def SIF_embedding(We, x, m, w, rmpc, dim):\n",
    "    \"\"\"\n",
    "    Compute the scores between pairs of sentences using weighted average + removing the projection on the first principal component\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in the i-th sentence\n",
    "    :param w: w[i, :] are the weights for the words in the i-th sentence\n",
    "    :param params.rmpc: if >0, remove the projections of the sentence embeddings to their first principal component\n",
    "    :return: emb, emb[i, :] is the embedding for sentence i\n",
    "    \"\"\"\n",
    "    emb = np.nan_to_num(get_weighted_average(We, x, m, w, dim))\n",
    "    if rmpc > 0:\n",
    "        emb = remove_pc(emb, rmpc)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "attempted-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(docs, all_data, model, words2idx, dim, rmpc=1):\n",
    "    \"\"\"\n",
    "    :param docs: list of strings (i.e. docs), based on which to do the tf-idf weighting.\n",
    "    :param all_data: dataframe column / list of strings (all tweets)\n",
    "    :param model: pretrained word vectors\n",
    "    :param vocab: a dictionary, words['str'] is the indices of the word 'str'\n",
    "    :param dim: dimension of embeddings\n",
    "    :param rmpc: number of principal components to remove\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(dim)\n",
    "\n",
    "    print('Getting word weights...')\n",
    "    #get the word weights\n",
    "    word2weight = get_word_weights(docs)\n",
    "    # load sentences\n",
    "    print('Loading sentences...')\n",
    "    #get the sentence/word weight indices\n",
    "    x, m, w = sentences2idx(all_data, words2idx, word2weight)  # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
    "    print('Creating embeddings...')\n",
    "    #create and return the sentence embeddings\n",
    "    return SIF_embedding(model, x, m, w, rmpc, dim)  # embedding[i,:] is the embedding for sentence i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "located-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the samples for computing the word weights\n",
    "def get_samples_for_computing_word_weights(p, sample_size):\n",
    "    tweets = []\n",
    "    with open('/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/' + p + '_clean/cleaned_text.txt', 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        tweets.extend([lines[i] for i in sorted(RNG.sample(range(len(lines)), min(sample_size, len(lines))))])\n",
    "    with open('/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/' + p +  '_tweets_for_weights.txt', 'w') as f:\n",
    "        f.write('\\n'.join(tweets))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "worth-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for cleaning text\n",
    "#set the stemmer\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "#list of punctuation to be removed\n",
    "punct_chars = list((set(string.punctuation) | {'’', '‘', '–', '—', '~', '|', '“', '”', '…', \"'\", \"`\", '_'}) - set(['#']))\n",
    "#sort the punctuation\n",
    "punct_chars.sort()\n",
    "#make puncutation one string\n",
    "punctuation = ''.join(punct_chars)\n",
    "#symbols to be removed\n",
    "replace = re.compile('[%s]' % re.escape(punctuation))\n",
    "\n",
    "##########################################\n",
    "######function for cleaning text##########\n",
    "##########################################\n",
    "def clean_text(text, event=None, stem=True):\n",
    "    #remove emojis\n",
    "    text = re.sub('<U\\+[^>]+>', '', text)\n",
    "    #replace &amp; with and\n",
    "    text = re.sub('&amp;', 'and', text)\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    # eliminate urls\n",
    "    text = re.sub(r'http\\S*|\\S*\\.com\\S*|\\S*www\\S*', ' ', text)\n",
    "    #eliminate @mentions\n",
    "    text = re.sub(r'\\s@\\S+', ' ', text)\n",
    "    # substitute all other punctuation with whitespace\n",
    "    text = replace.sub(' ', text)\n",
    "    # replace all whitespace with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # strip off spaces on either end\n",
    "    text = text.strip()\n",
    "    # stem words\n",
    "    words = text.split()\n",
    "    if stem:\n",
    "        words = [sno.stem(w) for w in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hourly-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for cleaning tweet and keeping words in vocab set\n",
    "def clean_tweet(text):\n",
    "    cleaned = clean_text(text)\n",
    "    return ' '.join([w for w in cleaned if w in vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "powered-reverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sentence embeddings for reps first\n",
    "#number of dimensions = 100\n",
    "d = 100\n",
    "\n",
    "#load in vectors\n",
    "vectors = Word2Vec.load('/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/rep_word2vec_100.model')\n",
    "\n",
    "#create an index that maps to the ordering of words in the word embedding vectors\n",
    "words2index = {w: i for i, w in enumerate(vectors.wv.vocab)}\n",
    "\n",
    "#for getting words from index\n",
    "index2word = {i: w for i, w in enumerate(vectors.wv.vocab)}\n",
    "\n",
    "#get sample of tweets for generating word weights, 30k * 8 months = 240K sample\n",
    "tweets_for_weights = get_samples_for_computing_word_weights(p = 'rep', sample_size = 240000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "limited-wright",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12612"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in vocab for cleaning the full text\n",
    "vocab = vectors.wv.vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "systematic-engineer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2655243\n",
      "2536232\n",
      "2801577\n",
      "2724289\n",
      "3073973\n",
      "3040480\n",
      "2274517\n",
      "1369799\n"
     ]
    }
   ],
   "source": [
    "#load the monthly data and filter to only republicans\n",
    "list_dfs = []\n",
    "\n",
    "for m in months:\n",
    "    df = pd.read_csv(INPUT + m + '_all_tweets.csv', encoding = 'UTF-8', dtype = 'str')\n",
    "    list_dfs.append(df[(df['dem_follows'] == '0') & (df['rep_follows'] == '1')]) #only append rep data\n",
    "    \n",
    "#print number of tweets in each dataframe\n",
    "for i, df in enumerate(list_dfs):\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "manufactured-wallace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08084081193322043\n",
      "0.08213956767361977\n",
      "0.08585914290415719\n",
      "0.08255658632399132\n",
      "0.09135311208003453\n",
      "0.09761715255485975\n",
      "0.09812676713341778\n",
      "0.08842903228867885\n"
     ]
    }
   ],
   "source": [
    "#keep only english tweets and remove the proportion of tweets removed for each month\n",
    "for i, df in enumerate(list_dfs):\n",
    "    #print proportion that is not english\n",
    "    print(len(df[df['lang'] != \"en\"]) / len(df))\n",
    "    #keep only english tweets in the list of dfs\n",
    "    list_dfs[i] = df[df['lang'] == \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "apparent-canyon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before cleaning: 2440591\n",
      "after cleaning: 2406714\n",
      "before cleaning: 2327907\n",
      "after cleaning: 2294805\n",
      "before cleaning: 2561036\n",
      "after cleaning: 2527290\n",
      "before cleaning: 2499381\n",
      "after cleaning: 2464728\n",
      "before cleaning: 2793156\n",
      "after cleaning: 2750488\n",
      "before cleaning: 2743677\n",
      "after cleaning: 2694886\n",
      "before cleaning: 2051326\n",
      "after cleaning: 2015834\n",
      "before cleaning: 1248669\n",
      "after cleaning: 1230169\n"
     ]
    }
   ],
   "source": [
    "#loop through dfs and clean text\n",
    "for i, df in enumerate(list_dfs):\n",
    "    list_dfs[i]['text'] = list_dfs[i]['text'].astype(str).apply(clean_tweet)\n",
    "    print('before cleaning: ' + str(len(list_dfs[i])))\n",
    "    #drop tweets that are whitespace\n",
    "    list_dfs[i] = df[df['text'].str.contains(' ')]\n",
    "    print('after cleaning: ' + str(len(list_dfs[i])))\n",
    "    \n",
    "    #save as csv\n",
    "    list_dfs[i].to_csv(OUTPUT + 'cleaned/rep_clean_' + str(i) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "level-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/trump_tweets.csv\", float_precision='round_trip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "moderate-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_id'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "royal-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str).apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "irish-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text'].str.contains(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aboriginal-pittsburgh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3141"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "opposed-stake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Getting word weights...\n",
      "Loading sentences...\n",
      "maxlen 60\n",
      "samples 3141\n",
      "0\n",
      "Creating embeddings...\n",
      "Getting weighted average...\n",
      "3141 100\n",
      "0\n",
      "Removing principal component...\n",
      "Computing principal components...\n"
     ]
    }
   ],
   "source": [
    "embedding = generate_embeddings(tweets_for_weights, df['text'], vectors, words2index, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "failing-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = pd.DataFrame(embedding, index = df['new_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "loaded-sitting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3141"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "driven-holly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.073564</td>\n",
       "      <td>0.081949</td>\n",
       "      <td>-0.005785</td>\n",
       "      <td>-0.202244</td>\n",
       "      <td>0.246758</td>\n",
       "      <td>0.218480</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>-0.028670</td>\n",
       "      <td>-0.133112</td>\n",
       "      <td>-0.132207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108177</td>\n",
       "      <td>0.024074</td>\n",
       "      <td>0.186911</td>\n",
       "      <td>-0.243940</td>\n",
       "      <td>-0.149253</td>\n",
       "      <td>0.043281</td>\n",
       "      <td>-0.117061</td>\n",
       "      <td>0.126005</td>\n",
       "      <td>0.273804</td>\n",
       "      <td>-0.105190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.033136</td>\n",
       "      <td>-0.045747</td>\n",
       "      <td>0.216323</td>\n",
       "      <td>0.122513</td>\n",
       "      <td>0.103753</td>\n",
       "      <td>0.171067</td>\n",
       "      <td>0.217599</td>\n",
       "      <td>-0.144914</td>\n",
       "      <td>0.141763</td>\n",
       "      <td>0.038024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180825</td>\n",
       "      <td>0.189626</td>\n",
       "      <td>0.030355</td>\n",
       "      <td>-0.510649</td>\n",
       "      <td>-0.180899</td>\n",
       "      <td>-0.039220</td>\n",
       "      <td>-0.027464</td>\n",
       "      <td>0.541595</td>\n",
       "      <td>-0.141667</td>\n",
       "      <td>-0.150322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.002573</td>\n",
       "      <td>-0.049544</td>\n",
       "      <td>0.649211</td>\n",
       "      <td>-0.144131</td>\n",
       "      <td>-0.586543</td>\n",
       "      <td>0.233466</td>\n",
       "      <td>0.154909</td>\n",
       "      <td>0.247567</td>\n",
       "      <td>-0.401007</td>\n",
       "      <td>0.441537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701509</td>\n",
       "      <td>0.043682</td>\n",
       "      <td>0.243500</td>\n",
       "      <td>0.072113</td>\n",
       "      <td>-0.056746</td>\n",
       "      <td>0.419754</td>\n",
       "      <td>-0.091610</td>\n",
       "      <td>0.596091</td>\n",
       "      <td>1.080192</td>\n",
       "      <td>0.297770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.056326</td>\n",
       "      <td>0.229281</td>\n",
       "      <td>-0.045270</td>\n",
       "      <td>-0.141668</td>\n",
       "      <td>0.082001</td>\n",
       "      <td>-0.028151</td>\n",
       "      <td>0.310029</td>\n",
       "      <td>0.332903</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>-0.033075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.332025</td>\n",
       "      <td>-0.080705</td>\n",
       "      <td>-0.216858</td>\n",
       "      <td>0.250835</td>\n",
       "      <td>0.250046</td>\n",
       "      <td>-0.351637</td>\n",
       "      <td>-0.078480</td>\n",
       "      <td>-0.005941</td>\n",
       "      <td>-0.074247</td>\n",
       "      <td>-0.082971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.382766</td>\n",
       "      <td>-0.081024</td>\n",
       "      <td>-0.189134</td>\n",
       "      <td>0.277608</td>\n",
       "      <td>0.182576</td>\n",
       "      <td>-0.232103</td>\n",
       "      <td>-0.536760</td>\n",
       "      <td>0.041914</td>\n",
       "      <td>0.477109</td>\n",
       "      <td>-0.564180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.242628</td>\n",
       "      <td>-0.780150</td>\n",
       "      <td>-0.382058</td>\n",
       "      <td>0.541565</td>\n",
       "      <td>-0.071992</td>\n",
       "      <td>-0.424914</td>\n",
       "      <td>0.306443</td>\n",
       "      <td>-0.364825</td>\n",
       "      <td>-0.537070</td>\n",
       "      <td>-0.018623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.092318</td>\n",
       "      <td>-0.610982</td>\n",
       "      <td>0.317876</td>\n",
       "      <td>-0.012196</td>\n",
       "      <td>0.071507</td>\n",
       "      <td>-0.467429</td>\n",
       "      <td>-0.463095</td>\n",
       "      <td>-0.283753</td>\n",
       "      <td>-0.115868</td>\n",
       "      <td>-0.113337</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157606</td>\n",
       "      <td>-0.041729</td>\n",
       "      <td>0.274131</td>\n",
       "      <td>-0.263535</td>\n",
       "      <td>-0.322494</td>\n",
       "      <td>-0.169191</td>\n",
       "      <td>0.265779</td>\n",
       "      <td>-0.052536</td>\n",
       "      <td>-0.094045</td>\n",
       "      <td>-0.211846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.394193</td>\n",
       "      <td>0.080165</td>\n",
       "      <td>0.034296</td>\n",
       "      <td>0.134974</td>\n",
       "      <td>-0.558802</td>\n",
       "      <td>-0.538120</td>\n",
       "      <td>-0.121854</td>\n",
       "      <td>-0.496141</td>\n",
       "      <td>0.017788</td>\n",
       "      <td>-0.017248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.458408</td>\n",
       "      <td>0.297960</td>\n",
       "      <td>0.822449</td>\n",
       "      <td>0.270187</td>\n",
       "      <td>-0.636212</td>\n",
       "      <td>0.401391</td>\n",
       "      <td>-0.156196</td>\n",
       "      <td>-0.409086</td>\n",
       "      <td>-0.679627</td>\n",
       "      <td>0.111035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.261800</td>\n",
       "      <td>0.117579</td>\n",
       "      <td>-0.177094</td>\n",
       "      <td>-0.079597</td>\n",
       "      <td>-0.190123</td>\n",
       "      <td>0.061346</td>\n",
       "      <td>-0.299552</td>\n",
       "      <td>-0.366490</td>\n",
       "      <td>-0.119471</td>\n",
       "      <td>0.165477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046086</td>\n",
       "      <td>0.133063</td>\n",
       "      <td>-0.182100</td>\n",
       "      <td>0.045974</td>\n",
       "      <td>0.066244</td>\n",
       "      <td>-0.367505</td>\n",
       "      <td>-0.302448</td>\n",
       "      <td>-0.171390</td>\n",
       "      <td>-0.103820</td>\n",
       "      <td>0.406921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.464115</td>\n",
       "      <td>-0.137322</td>\n",
       "      <td>-0.259617</td>\n",
       "      <td>0.003323</td>\n",
       "      <td>0.045364</td>\n",
       "      <td>0.068199</td>\n",
       "      <td>-0.183191</td>\n",
       "      <td>-0.049691</td>\n",
       "      <td>0.096815</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054176</td>\n",
       "      <td>-0.094137</td>\n",
       "      <td>0.402045</td>\n",
       "      <td>-0.153957</td>\n",
       "      <td>-0.339999</td>\n",
       "      <td>0.266362</td>\n",
       "      <td>-0.133496</td>\n",
       "      <td>0.238312</td>\n",
       "      <td>-0.633142</td>\n",
       "      <td>0.110635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.102499</td>\n",
       "      <td>0.186191</td>\n",
       "      <td>-1.182891</td>\n",
       "      <td>-0.113912</td>\n",
       "      <td>-0.786610</td>\n",
       "      <td>0.751293</td>\n",
       "      <td>0.619332</td>\n",
       "      <td>0.322484</td>\n",
       "      <td>-0.945972</td>\n",
       "      <td>-0.139472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593449</td>\n",
       "      <td>-1.151752</td>\n",
       "      <td>0.342510</td>\n",
       "      <td>0.258641</td>\n",
       "      <td>0.402657</td>\n",
       "      <td>0.225254</td>\n",
       "      <td>-0.215405</td>\n",
       "      <td>1.653555</td>\n",
       "      <td>0.699425</td>\n",
       "      <td>-0.061285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "new_id                                                                         \n",
       "0       0.073564  0.081949 -0.005785 -0.202244  0.246758  0.218480  0.007749   \n",
       "1      -0.033136 -0.045747  0.216323  0.122513  0.103753  0.171067  0.217599   \n",
       "2      -0.002573 -0.049544  0.649211 -0.144131 -0.586543  0.233466  0.154909   \n",
       "3       0.056326  0.229281 -0.045270 -0.141668  0.082001 -0.028151  0.310029   \n",
       "4       0.382766 -0.081024 -0.189134  0.277608  0.182576 -0.232103 -0.536760   \n",
       "5      -0.092318 -0.610982  0.317876 -0.012196  0.071507 -0.467429 -0.463095   \n",
       "6      -0.394193  0.080165  0.034296  0.134974 -0.558802 -0.538120 -0.121854   \n",
       "7      -0.261800  0.117579 -0.177094 -0.079597 -0.190123  0.061346 -0.299552   \n",
       "8      -0.464115 -0.137322 -0.259617  0.003323  0.045364  0.068199 -0.183191   \n",
       "9      -1.102499  0.186191 -1.182891 -0.113912 -0.786610  0.751293  0.619332   \n",
       "\n",
       "              7         8         9   ...        90        91        92  \\\n",
       "new_id                                ...                                 \n",
       "0      -0.028670 -0.133112 -0.132207  ...  0.108177  0.024074  0.186911   \n",
       "1      -0.144914  0.141763  0.038024  ... -0.180825  0.189626  0.030355   \n",
       "2       0.247567 -0.401007  0.441537  ...  0.701509  0.043682  0.243500   \n",
       "3       0.332903  0.059105 -0.033075  ... -0.332025 -0.080705 -0.216858   \n",
       "4       0.041914  0.477109 -0.564180  ... -0.242628 -0.780150 -0.382058   \n",
       "5      -0.283753 -0.115868 -0.113337  ... -0.157606 -0.041729  0.274131   \n",
       "6      -0.496141  0.017788 -0.017248  ... -0.458408  0.297960  0.822449   \n",
       "7      -0.366490 -0.119471  0.165477  ...  0.046086  0.133063 -0.182100   \n",
       "8      -0.049691  0.096815  0.001571  ... -0.054176 -0.094137  0.402045   \n",
       "9       0.322484 -0.945972 -0.139472  ...  0.593449 -1.151752  0.342510   \n",
       "\n",
       "              93        94        95        96        97        98        99  \n",
       "new_id                                                                        \n",
       "0      -0.243940 -0.149253  0.043281 -0.117061  0.126005  0.273804 -0.105190  \n",
       "1      -0.510649 -0.180899 -0.039220 -0.027464  0.541595 -0.141667 -0.150322  \n",
       "2       0.072113 -0.056746  0.419754 -0.091610  0.596091  1.080192  0.297770  \n",
       "3       0.250835  0.250046 -0.351637 -0.078480 -0.005941 -0.074247 -0.082971  \n",
       "4       0.541565 -0.071992 -0.424914  0.306443 -0.364825 -0.537070 -0.018623  \n",
       "5      -0.263535 -0.322494 -0.169191  0.265779 -0.052536 -0.094045 -0.211846  \n",
       "6       0.270187 -0.636212  0.401391 -0.156196 -0.409086 -0.679627  0.111035  \n",
       "7       0.045974  0.066244 -0.367505 -0.302448 -0.171390 -0.103820  0.406921  \n",
       "8      -0.153957 -0.339999  0.266362 -0.133496  0.238312 -0.633142  0.110635  \n",
       "9       0.258641  0.402657  0.225254 -0.215405  1.653555  0.699425 -0.061285  \n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "given-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.to_csv(\"/home/twalton_umass_edu/Political Polarization Project/tmls/word_embeddings/trump_tweets_embeds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "treated-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "del list_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-sherman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Getting word weights...\n",
      "Loading sentences...\n",
      "maxlen 90\n",
      "samples 2406714\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "Creating embeddings...\n",
      "Getting weighted average...\n",
      "2406714 100\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "100\n",
      "Getting word weights...\n",
      "Loading sentences...\n",
      "maxlen 119\n",
      "samples 2294805\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "Creating embeddings...\n",
      "Getting weighted average...\n",
      "2294805 100\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "100\n",
      "Getting word weights...\n",
      "Loading sentences...\n",
      "maxlen 83\n",
      "samples 2527290\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "Creating embeddings...\n",
      "Getting weighted average...\n",
      "2527290 100\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "100\n",
      "Getting word weights...\n",
      "Loading sentences...\n",
      "maxlen 91\n",
      "samples 2464728\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "Creating embeddings...\n",
      "Getting weighted average...\n",
      "2464728 100\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "Removing principal component...\n",
      "Computing principal components...\n",
      "100\n",
      "Getting word weights...\n",
      "Loading sentences...\n",
      "maxlen 150\n",
      "samples 2750488\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "Creating embeddings...\n",
      "Getting weighted average...\n",
      "2750488 100\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    df = pd.read_csv(OUTPUT + 'cleaned/rep_clean_' + str(i) + '.csv')\n",
    "    embedding = generate_embeddings(tweets_for_weights, df['text'], vectors, words2index, d)\n",
    "    embedding = pd.DataFrame(embedding, index = df.index)\n",
    "    embedding.to_csv(OUTPUT + 'trained/rep_trained_' + str(i) + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-python_work]",
   "language": "python",
   "name": "conda-env-.conda-python_work-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
